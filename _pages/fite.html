---
layout: default
title: "FITE"
permalink: /fite/
---

<div class="container-fluid">
  <div class="row">
    <nav id="sidebar" class="col-md-3 col-lg-2 d-md-block bg-light sidebar">
      <div class="position-sticky">
        <ul class="nav flex-column">
          <li class="nav-item">
            <a class="nav-link active" href="#about-this-program">1. About This Program</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#technical-sharing">2. Technical Sharing</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-1">2.1 Share 1</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-2">2.2 Share 2</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-3">2.3 Share 3</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#llama-tutorial">3. Llama Tutorial</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#deployment">3.1 Deployment</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#fine-tune">3.2 Fine-tune</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#gpt-programming-interface">4. GPT Programming Interface</a></li>
          <li class="nav-item">
            <a class="nav-link" href="#qa">5. Q&A</a></li>
          <li class="nav-item">
            <a class="nav-link" href="#contact">6. Contact</a></li>
        </ul>
      </div>
    </nav>

    <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
      <div id="about-this-program" class="section">
        <h2>1. About This Program</h2>
        <p>This project is supported by the Fund for Innovative Technology-in-Education (FITE) allocated by the University Grants Committee (UGC). We aim to provide interested students with hands-on learning experience about recent technical breakthroughs on large language models (LLMs). This webpage provides an important platform that works as 1) an active learning community, and 2) a sandbox environment for API-based LLM interactions.</p>
        <p>We will periodically update the latest technical evolutions of LLMs on this learning platform. And we have also posted entry-level tutorial materials on the platform to help new learners begin easily. Additionally, discussion and Q&A are highly welcomed within the community.</p>
        <p>Further, the platform also provides users with a sandbox environment where they can experience LLM programming via an API so that they can gain hands-on LLM programming experience without the need for expensive GPUs or a complex local environment setup.</p>
      </div>
      <hr>

      <div id="technical-sharing" class="section">
        <h2>2. Technical Sharing</h2>
        <div id="share-1" class="section">
          <h3>2.1 Share 1</h3>
          <h4>Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth</h4>
          <p><img src="/images/respic/share1.jpg" class="img-responsive" width="50%" /></p>
          <p>The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed-source and open-weight models. Instead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you can fine-tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost.</p>
          <p><a href="https://huggingface.co/blog/mlabonne/sft-llama3" target="_blank">Click here for details</a></p>
          <div style="margin-top: 20px;">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/rpAtVIZB72U" frameborder="0" allowfullscreen></iframe>
          </div>
        </div>
        <hr>
        
        <div id="share-2" class="section">
          <h3>2.2 Share 2</h3>
          <h4>Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora</h4>
          <p><img src="/images/respic/share2.jpg" class="img-responsive" width="60%" /></p>
          <p>Open LLMs like Meta Llama 3, Mistral AI Mistral & Mixtral models or AI21 Jamba are now OpenAI competitors. However, most of the time you need to fine-tune the model on your data to unlock the full potential of the model. Fine-tuning smaller LLMs, like Mistral became very accessible on a single GPU by using Q-Lora. But efficiently fine-tuning bigger models like Llama 3 70b or Mixtral stayed a challenge until now.</p>
          <p>This blog post walks you thorugh how to fine-tune a Llama 3 using PyTorch FSDP and Q-Lora with the help of Hugging Face TRL, Transformers, peft & datasets. In addition to FSDP we will use Flash Attention v2 through the Pytorch SDPA implementation.</p>
          <p><a href="https://www.philschmid.de/fsdp-qlora-llama3" target="_blank">Click here for details</a></p>
        </div>
        <hr>
        
        <div id="share-3" class="section">
          <h3>2.3 Share 3</h3>
          <h4>The Ultimate Guide to Fine-Tune LLaMA 3, With LLM Evaluations</h4>
          <p><img src="/images/respic/share3.jpg" class="img-responsive" width="50%" /></p>
          <p>Fine-tuning a Large Language Model (LLM) comes with tons of benefits when compared to relying on proprietary foundational models such as OpenAI’s GPT models. Think about it, you get 10x cheaper inference cost, 10x faster tokens per second, and not have to worry about any shady stuff OpenAI’s doing behind their APIs. The way everyone should be thinking about fine-tuning, is not how we can outperform OpenAI or replace RAG, but how we can maintain the same performance while cutting down on inference time and cost for your specific use case.</p>
          <p>But let’s face it, the average Joe building RAG applications isn’t confident in their ability to fine-tune an LLM — training data are hard to collect, methodologies are hard to understand, and fine-tuned models are hard to evaluate. And so, fine-tuning has became the best vitamin for LLM practitioners. You’ll often hear excuses such as “Fine-tuning isn’t a priority right now”, “We’ll try with RAG and move to fine-tuning if necessary”, and the classic “Its on the roadmap”. But what if I told you anyone could get started with fine-tuning an LLM in under 2 hours, for free, in under 100 lines of code? Instead of RAG or fine-tuning, why not both?</p>
          <p><a href="https://www.confident-ai.com/blog/the-ultimate-guide-to-fine-tune-llama-2-with-llm-evaluations" target="_blank">Click here for details</a></p>
      </div>
      <hr>
        
      <div id="llama-tutorial" class="section">
        <h2>3. Llama Tutorial</h2>
        <div id="deployment" class="section">
          <h3>3.1 Deployment</h3>
          <p>LLaMA 3 (Large Language Model Meta AI 3) is the third generation of large language model developed by Meta (Facebook). It is an advanced model based on deep learning and artificial intelligence technology, designed to generate high-quality natural language text. LLaMA 3 is commonly used in various natural language processing tasks, including text generation, translation, question answering, text summarization, etc. As an upgraded version after LLaMA 2, it has improved performance, efficiency and processing power, and can better understand and generate complex language structures.</p>
          <p>The LlaMA3 series model group supports 8B and 70B pre-trained versions, and is open source for everyone to use. In the following tutorials, we will deploy and use the dataset to fine-tune the LoRA of llaMA3-8B.</p>
          <p><strong>&lt;1&gt; Deploy and Run on Linux</strong></p>
          <p>With a Linux setup that has a GPU with at least 16GB of VRAM, you should be able to load the 8B Llama model in fp16 natively. If you have an Nvidia GPU, you can confirm your setup using the NVIDIA System Management Interface tool, which will show you what GPU you have, available VRAM, and other useful information,  </p>
          <p>by typing: <code>nvidia-smi</code></p>
          <p>to view nvidia's VRAM configuration.</p>
          <p><img src="/images/respic/tut1.png" class="img-responsive" width="80%" /></p>
          <p><strong>&lt;2&gt; Get llama3 Weight</strong></p>
          <p>Visit the <a href="https://llama.meta.com/llama-downloads/" target="_blank">llama website</a>, fill in your details in the form, and select the model you want to download.</p>
          <p>Read and agree to the license agreement, then click Accept and Continue. You will see a unique URL on the website. You will also receive the URL in an email, which will be valid for 24 hours and allow you to download each model up to 5 times. You can request a new URL at any time.</p>
          <p>Now, we are ready to get the weights and run the model locally on our machine. It is recommended to use a Python virtual environment to run this demo. In this demo, we use Miniconda, but you can use any virtual environment of your choice. In addition, please refer to the configuration instructions of <a href="https://docs.anaconda.com/miniconda/miniconda-install/" target="_blank">miniconda</a>:</p>
          <p><img src="/images/respic/tut2.png" class="img-responsive" width="80%" /></p>
          <p>Open your terminal and create a new folder called llama3-demo in your workspace. Navigate to the new folder and clone the Llama repository:</p>
          <p><code>mkdir llama3-demo</code></p>
          <p><code>cd llama3-demo</code></p>
          <p><code>git clone https://github.com/meta-llama/llama3.git</code></p>
          <p><img src="/images/respic/tut3.png" class="img-responsive" width="80%" /></p>
          <p>For this demo, we need to install two prerequisites: wget and md5sum. To confirm if your distribution has these, use:</p>
          <p><code>wget --version</code></p>
          <p><code>md5sum –version</code></p>
          <p>It should return the installed version. If your distribution does not have these, you can install them with the following commands:</p>
          <p><code>apt-get install wget</code></p>
          <p><code>apt-get install md5sum</code></p>
          <p>To make sure we have all package dependencies installed, in the newly cloned repo folder enter:</p>
          <p><code>pip install -e </code></p>
          <p><img src="/images/respic/tut4.png" class="img-responsive" width="80%" /></p>
          <p>Now, we are ready to download the model weights for our local setup. Our team created a helper script to make it easy to download the model weights. In your terminal, enter:</p>
          <p><code>./download.sh</code></p>
          <p><img src="/images/respic/tut5.png" class="img-responsive" width="80%" /></p>
          <p>The script will ask you for the URL from your email. Paste the URL you received from Meta. It will then ask you to enter a list of models to download. In our example, we will download the 8B pretrained model and the fine-tuned 8B chat model. Therefore, we will enter "8B,8B-instruct".</p>
          <p><strong>&lt;3&gt; Running the Model</strong></p>
          <p>We are ready to run an example inference script to test that our model is setup correctly and working properly. We can use an example Python script called <a href="https://github.com/meta-llama/llama3/blob/main/example_text_completion.py" target="_blank">example_text_completion.py</a> to test the model. </p>
          <p>To run the script, return to our terminal and type in the llama3repo:</p>
          <p><code>torchrun --nproc_per_node 1 example_text_completion.py </code></p>
          <p><code>-ckpt_dir Meta-Llama-3-8B/ </code></p>
          <p><code>-tokenizer_path Meta-Llama-3-8B/tokenizer.model </code></p>
          <p><code>-max_seq_len 128 </code></p>
          <p><code>-max_batch_size 4</code></p>
          <p>Replace Meta-Llama-3-8B/ with the path to your checkpoint directory and tokenizer.model with the path to your tokenizer model. If you run it from this main directory, the paths may not need to be changed.</p>
          <p>Set –nproc_per_node to the MP value for the model you are using. For the 8B model, this value is set to 1.</p>
          <p>Adjust the max_seq_len and max_batch_size parameters as needed. We set them to 128 and 4 respectively. </p>
          <p><img src="/images/respic/tut6.png" class="img-responsive" width="80%" /></p>
          <p><strong>&lt;4&gt; Trying out and Improving 8b-instruct</strong></p>
          <p>To try out the fine-tuned chat model (8B-instruct), we have a similar example called <a href="https://github.com/meta-llama/llama3/blob/main/example_chat_completion.py" target="_blank">example_chat_completion.py</a>.</p>
          <p><code>torchrun --nproc_per_node 1 example_chat_completion.py </code></p>
          <p><code>-ckpt_dir Meta-Llama-3-8B-Instruct/ </code></p>
          <p><code>-tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model </code></p>
          <p><code>-max_seq_len 512 </code></p>
          <p><code>-max_batch_size 6</code></p>
          <p><img src="/images/respic/tut7.png" class="img-responsive" width="80%" /></p>
          <p><img src="/images/respic/tut8.png" class="img-responsive" width="80%" /></p>
          <p>Note that in this case, we use the Meta-Llama-3-8B-Instruct/model and provide the correct tokenizer instruction under the model folder.</p>
        </div>
        <hr>
        <div id="fine-tune" class="section">
          <h3>3.2 Fine-tune</h3>
          <p><strong>&lt;1&gt; Recipes PEFT LoRA</strong></p>
          <p>The llama-recipes repo has details on different fine-tuning (FT) alternatives supported by the provided sample scripts. In particular, it highlights the use of PEFT as the preferred FT method, as it reduces the hardware requirements and prevents catastrophic forgetting. For specific cases, full parameter FT can still be valid, and different strategies can be used to still prevent modifying the model too much. Additionally, FT can be done in single gpu or multi-gpu with FSDP.</p>
          <p>In order to run the recipes, follow the steps below:</p>
          <p>Create a conda environment with pytorch and additional dependencies</p>
          <p>Install the recipes as described <a href="https://github.com/meta-llama/llama-recipes#install-with-pip" target="_blank">here</a>:</p>
          <p><img src="/images/respic/tut9.png" class="img-responsive" width="80%" /></p>
          <p>Download the desired model from hf, either using git-lfs or using the llama download script.</p>
          <p>With everything configured, run the following command:</p>
          <p><code>python -m llama_recipes.finetuning \</code></p>
          <p><code>-use_peft -peft_method lora -quantization \</code></p>
          <p><code>-model_name ../llama/models_hf/8B \</code></p>
          <p><code>-output_dir ../llama/models_ft/8B-peft \</code></p>
          <p><code>-batch_size_training 2 -gradient_accumulation_steps 2</code></p>
          <p><strong>&lt;2&gt; Torchtune</strong></p>
          <p><a href="https://github.com/pytorch/torchtune" target="_blank">Torchtune</a> is a PyTorch-native library for fine-tuning Meta Llama family models (including Meta Llama 3). It supports an end-to-end fine-tuning lifecycle, including:</p>
          <p>• Download model checkpoints and datasets</p>
          <p>• Training regimens for fine-tuning Llama 3 using full fine-tuning, LoRA, and QLoRA</p>
          <p>• Support for single GPU fine-tuning, running on consumer GPUs with 24GB VRAM</p>
          <p>• Scale fine-tuning to multiple GPUs using PyTorch FSDP</p>
          <p>• Log metrics and model checkpoints during training using weights and biases</p>
          <p>• Evaluate fine-tuned models using EleutherAI’s LM evaluation tool</p>
          <p>• Post-training quantization of fine-tuned models via TorchAO</p>
          <p>• Interoperability with inference engines including ExecuTorch</p>
          <p>To install torchtune, simply run the pip install command</p>
          <p>pip install torchtune<code></code></p>
          <p><strong>&lt;3&gt; Hugging Face Llama 3 Model Weights</strong></p>
          <p>Follow the instructions on the Hugging Face <a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B" target="_blank">meta-llama</a> repository to make sure you have access to the Llama 3 model weights. Once you have confirmed you have access, you can run the following command to download the weights to your local machine. This will also download the tokenizer model and responsible use guide. </p>
          <p><img src="/images/respic/tut10.png" class="img-responsive" width="60%" /></p>
          <p><code>tune download meta-llama/Meta-Llama-3-8B \</code></p>
          <p><code>-output-dir <checkpoint_dir> \</code></p>
          <p><code>-hf-token <ACCESS TOKEN></code></p>
          <p>Set the environment variable HF_TOKEN or pass --hf-token to the command to verify your access. You can find your token at <a href="https://huggingface.co/settings/tokens" target="_blank">here</a>.</p>
          <p>After using hugging face to download the weights locally, we write a script to run LoRA fine-tuning. The database used is meta's official IMDB dataset, and the running time is about two and a half hours.</p>
          <p><strong>&lt;4&gt; Example LoRA Script</strong></p>
          <p>You can use the following basic LoRA fine-tuning script template as a reference, including LoRA configuration, data preprocessing, training parameter settings, model fine-tuning, and saving related parts.</p>
          <pre><code>
from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from peft import LoraConfig, get_peft_model
import torch

# Load the model and tokenizer
model_name = "meta-llama/Meta-Llama-3-8B"
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float32,
    device_map="auto"
)
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"

# Load the dataset
dataset = load_dataset("imdb", split='train[:1%]')
eval_dataset = load_dataset("imdb", split='test[:1%]')

# Configure LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none"
)
model = get_peft_model(base_model, lora_config)

# Data preprocessing function
def preprocess_function(examples):
    tokenized_inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=512)
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)
tokenized_eval_dataset = eval_dataset.map(preprocess_function, batched=True)

# Set training parameters
training_args = TrainingArguments(
    output_dir="./llama3-8b-lora-finetuned",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=2,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=10,
    eval_strategy="steps",
    eval_steps=10,
    fp16=False,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss"
)

# Creating and running the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    eval_dataset=tokenized_eval_dataset
)

trainer.train()

# Save the fine-tuned model
model.save_pretrained("./llama3-8b-lora-finetuned")
</code></pre>
          <p>To help the debugging and execution process go more smoothly. We can also add:</p>
          <p>• Logging and error handling: ensure that errors can be captured during code execution and provide debugging information.</p>
          <p>• Access token management: authentication when interacting with Hugging Face's API.</p>
          <p>• Custom Trainer class: to meet specific training needs.</p>
          <p>• Data preprocessing logic: convert raw data into a format acceptable to the model.</p>
          <p>• Optimize settings for device use: optimize code execution for hardware resources.</p>
          <p>These contents can help the code be more robust and flexible in actual applications, but when standardizing, you may choose to remove or simplify these parts to highlight the core logic.</p>
          <p>You can view this more detailed example <a href="/_pages/llama3_lora.py" download="llama3_lora.py">script</a>.</p>
          <p>Here are some key points to check if the LoRA fine-tuning script runs successfully:</p>
          <p>1. Model and marker are loaded successfully: INFO:__main__: Load the model and marker.</p>
          <p>2. Dataset is loaded successfully: INFO:__main__: Load the dataset.</p>
          <p>3. LoRA configuration is successful: INFO:__main__: Configure LoRA.</p>
          <p>4. Dataset preprocessing is successful: INFO:__main__: Preprocess the dataset.</p>
          <p>5. Training is normal: Loss and evaluation loss (eval_loss) are recorded at a certain number of steps during training.</p>
          <p>6. Model is saved successfully: INFO:__main__: Save the model.</p>
          <p><img src="/images/respic/tut11.jpg" class="img-responsive" width="80%" /></p>
          <p><img src="/images/respic/tut12.jpg" class="img-responsive" width="80%" /></p>
          <p><strong>&lt;5&gt; Summary and Next Steps</strong></p>
          <p>Based on this information, the following conclusions can be drawn:</p>
          <p>1. The training script ran successfully: The training script was not interrupted and successfully completed all training steps.</p>
          <p>2. The model was saved successfully: After training, the model was successfully saved.</p>
          <p>3. The loss gradually decreased: During the training and evaluation process, the loss gradually decreased, indicating that the model is gradually learning and improving.</p>
          <p>You can now proceed to the next step, such as:</p>
          <p>1. Evaluate the model: Evaluate the model using an independent test set to confirm the generalization ability of the model.</p>
          <p>2. Model inference: Use the fine-tuned model for inference tasks to verify its performance in actual applications.</p>
          <p>3. Further optimization: If necessary, you can further adjust the hyperparameters or use more data for fine-tuning to further improve the model performance.</p>
          <p>A possible example inference script is <a href="/_pages/infer_fintuned.py" download="llama3_lora.py">here</a>, and you can improve it according to your needs.</p>
        </div>
      </div>
        <hr>
        
        <div id="gpt-programming-interface" class="section">
<h2>4. GPT Programming Interface</h2>
<p>Please register with your CUHK account before using this feature.</p>

<button id="register-btn" style="padding: 10px 20px; background-color: #4CAF50; color: white; font-size: 16px; border: none; border-radius: 5px; cursor: pointer; margin-bottom: 20px; display: block;">Register</button>

<div id="overlay" style="display:none; position:fixed; top:0; left:0; width:100%; height:100%; background-color:rgba(0,0,0,0.5); z-index:999;"></div>

<div id="registration-popup" style="display:none; position:fixed; top:50%; left:50%; transform:translate(-50%, -50%); width:500px; padding:20px; background-color:white; border:1px solid #ccc; border-radius:5px; z-index:1000;">
    <label for="cuhk-account">Enter your CUHK account (If you are an external visitor, please enter the ten-digit '0') :</label>
    <input type="text" id="cuhk-account" placeholder="CUHK Account" style="width:100%; padding:10px; margin-top:10px; border-radius:5px; border:1px solid #ccc; font-size:16px;">
    <button id="confirm-registration" style="padding: 10px 20px; background-color: #4CAF50; color: white; font-size: 16px; border: none; border-radius: 5px; cursor: pointer; margin-top: 10px; display: block; width: 100%;">Confirm</button>
</div>

<p>Please configure the settings and interact with GPTs:</p>

<label for="prompt">Enter your prompt:</label>
<input type="text" id="prompt" placeholder="Type your prompt here..." style="width: 50%; padding: 10px; margin-top: 5px; margin-bottom: 20px; border-radius: 5px; border: 1px solid #ccc; font-size: 16px; display: block;">

<label for="model">Select a model:</label>
<select id="model" style="width: 50%; padding: 10px; margin-bottom: 20px; border-radius: 5px; border: 1px solid #ccc; font-size: 16px; background-color: #f9f9f9; appearance: none; display: block;">
    <option value="GPT-4o-mini">GPT-4o-mini rate = 0.50</option>
    <option value="Gemma-7b-it">Gemma-7b-it rate = 0.40</option>
    <option value="GPT-3.5-turbo">GPT-3.5-turbo rate = 0.75</option>
    <option value="Claude-3-haiku">Claude-3-haiku rate = 0.50</option>
    <option value="Deepseek-coder-6.7b">Deepseek-coder-6.7b rate = 0.60</option>
    <option value="Llama3-70b">Llama3-70b rate = 1.00</option>
</select>

<label for="temperature">Set temperature (0-1):</label>
<input type="number" id="temperature" min="0" max="1" step="0.01" value="0.5" style="width: 50%; padding: 10px; margin-bottom: 20px; border-radius: 5px; border: 1px solid #ccc; font-size: 16px; display: block;">

<label for="max_tokens">Set max tokens:</label>
<input type="number" id="max_tokens" value="100" style="width: 50%; padding: 10px; margin-bottom: 20px; border-radius: 5px; border: 1px solid #ccc; font-size: 16px; display: block;">

<button id="gpt-submit" onclick="sendToGPT()" style="padding: 10px 20px; background-color: #4CAF50; color: white; font-size: 16px; border: none; border-radius: 5px; cursor: pointer; margin-bottom: 30px; display: block;" disabled>Submit</button>

<h3>Response</h3>
<div id="gpt-output" style="width: 50%; min-height: 150px; padding: 15px; background-color: #f1f1f1; border-radius: 5px; border: 1px solid #ccc;"></div>

<script>
  document.getElementById('register-btn').addEventListener('click', function() {
    document.getElementById('overlay').style.display = 'block';
    document.getElementById('registration-popup').style.display = 'block';
  });

  document.getElementById('confirm-registration').addEventListener('click', async function() {
    const cuhkAccount = document.getElementById('cuhk-account').value;
    const cuhkAccountRegex = /^\d{10}$/;

    if (cuhkAccountRegex.test(cuhkAccount)) {
      try {
        const response = await fetch('http://localhost:5000/register', {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json'
          },
          body: JSON.stringify({ cuhk_account: cuhkAccount })
        });

        const data = await response.json();

        if (response.ok) {
          alert(data.message);
          document.getElementById('gpt-submit').disabled = false;
        } else {
          alert(data.message);
        }
      } catch (error) {
        alert('Registration failed. Please try again.');
      }
    } else {
      alert('Invalid account. Please try again.');
    }

    document.getElementById('cuhk-account').value = '';
    document.getElementById('registration-popup').style.display = 'none';
    document.getElementById('overlay').style.display = 'none';
  });

  async function sendToGPT() {
    const prompt = document.getElementById('prompt').value;
    const model = document.getElementById('model').value;
    const temperature = parseFloat(document.getElementById('temperature').value);
    const max_tokens = parseInt(document.getElementById('max_tokens').value);

    const requestData = {
      model: model,
      temperature: temperature,
      max_tokens: max_tokens,
      messages: [{"role": "user", "content": prompt}]
    };

    try {
      const response = await fetch('http://localhost:5000/generate', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(requestData)
      });

      const data = await response.json();

      if (response.ok) {
        const gptOutput = data.output;
        document.getElementById('gpt-output').innerText = gptOutput;
      } else {
        document.getElementById('gpt-output').innerText = `Error: ${data.error}`;
      }

    } catch (error) {
      document.getElementById('gpt-output').innerText = `Error: ${error.message}`;
    }
  }
</script>




      </div>
      
        <hr>
      <div id="qa" class="section">
        <h2>5. Q&A</h2>
        <p>We always welcome any questions and suggestions from you!</p>
      </div>
        <hr>
      <div id="contact" class="section">
        <h2>6. Contact</h2>
        <p>This page is developed and maintained by Harmin Chee. If you have any questions, please feel free to contact <a href="mailto:h.chee@link.cuhk.edu.hk">him</a>.</p>
      </div>
    </main>
  </div>
</div>

<style>
  #sidebar {
    position: fixed;
    top: 70px; /* Adjust this value to move the sidebar down */
    left: 0;
    width: 12%;
    height: 100%;
    z-index: 1;
  }
  main {
    margin-left: -15%;/* Increase margin to move content away from the sidebar */
    width: 130%;
  }
  #sidebar ul {
    padding-left: 0;
  }
  #sidebar .nav-item {
    list-style: none;
    margin-bottom: 10px; /* Add space between items */
  }
  #sidebar .nav-link {
    font-weight: bold; /* Make font bold */
    font-size: 1.1em; /* Increase font size */
    color: #333; /* Dark gray color */
  }
  .sub-item {
    padding-left: 20px;
  }
  .section h2, .section h3, .section h4 {
    text-align: left; /* Align titles to the left */
    margin-left: 0;
    font-weight: bold;
    color: #000; /* Set title color to black */
  }
  .section h2 {
    font-size: 2em; /* Increase size for h2 */
    margin-bottom: 0.75em; /* Increase spacing below h2 */
  }
  .section h3 {
    font-size: 1.75em; /* Increase size for h3 */
    margin-bottom: 0.6em; /* Increase spacing below h3 */
  }
  .section h4 {
    font-size: 1.5em; /* Increase size for h4 */
    margin-bottom: 0.5em; /* Increase spacing below h4 */
  }
  .section {
    padding-left: 0; /* Remove padding to align sections with the left */
    padding-right: 0;
  }
  .section p {
    text-align: left;
  }
</style>
