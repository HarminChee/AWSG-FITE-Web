---
layout: default
title: "FITE"
permalink: /fite/
---

<div class="container-fluid">
  <div class="row">
    <nav id="sidebar" class="col-md-3 col-lg-2 d-md-block bg-light sidebar">
      <div class="position-sticky">
        <ul class="nav flex-column">
          <li class="nav-item">
            <a class="nav-link active" href="#about-fite-program">1. About FITE Program</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#technical-sharing">2. Technical Sharing</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-1">2.1 Share 1</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-2">2.2 Share 2</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-3">2.3 Share 3</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#llama-tutorial">3. Llama Tutorial</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#deployment">3.1 Deployment</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#fine-tune">3.2 Fine-tune</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#gpt-programming-interface">4. GPT Programming Interface</a></li>
          <li class="nav-item">
            <a class="nav-link" href="#qa">5. Q&A</a></li>
          <li class="nav-item">
            <a class="nav-link" href="#contact">6. Contact</a></li>
        </ul>
      </div>
    </nav>

    <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
      <div id="about-fite-program" class="section">
        <h2>1. About FITE Program</h2>
        <p>...</p>
      </div>
      <hr>

      <div id="technical-sharing" class="section">
        <h2>2. Technical Sharing</h2>
        <div id="share-1" class="section">
          <h3>2.1 Share 1</h3>
          <h4>Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth</h4>
          <p><img src="/images/respic/share1.jpg" class="img-responsive" width="60%" /></p>
          <p>The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed-source and open-weight models. Instead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you can fine-tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost.</p>
          <p><a href="https://huggingface.co/blog/mlabonne/sft-llama3" target="_blank">Click here for details</a></p>
          <div style="margin-top: 20px;">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/rpAtVIZB72U" frameborder="0" allowfullscreen></iframe>
          </div>
        </div>
        <hr>
        
        <div id="share-2" class="section">
          <h3>2.2 Share 2</h3>
          <h4>Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora</h4>
          <p><img src="/images/respic/share2.jpg" class="img-responsive" width="60%" /></p>
          <p>Open LLMs like Meta Llama 3, Mistral AI Mistral & Mixtral models or AI21 Jamba are now OpenAI competitors. However, most of the time you need to fine-tune the model on your data to unlock the full potential of the model. Fine-tuning smaller LLMs, like Mistral became very accessible on a single GPU by using Q-Lora. But efficiently fine-tuning bigger models like Llama 3 70b or Mixtral stayed a challenge until now.</p>
          <p>This blog post walks you thorugh how to fine-tune a Llama 3 using PyTorch FSDP and Q-Lora with the help of Hugging Face TRL, Transformers, peft & datasets. In addition to FSDP we will use Flash Attention v2 through the Pytorch SDPA implementation.</p>
          <p><a href="https://www.philschmid.de/fsdp-qlora-llama3" target="_blank">Click here for details</a></p>
        </div>
        <hr>
        
        <div id="share-3" class="section">
          <h3>2.3 Share 3</h3>
          <h4>The Ultimate Guide to Fine-Tune LLaMA 3, With LLM Evaluations</h4>
          <p><img src="/images/respic/share3.jpg" class="img-responsive" width="50%" /></p>
          <p>Fine-tuning a Large Language Model (LLM) comes with tons of benefits when compared to relying on proprietary foundational models such as OpenAI’s GPT models. Think about it, you get 10x cheaper inference cost, 10x faster tokens per second, and not have to worry about any shady stuff OpenAI’s doing behind their APIs. The way everyone should be thinking about fine-tuning, is not how we can outperform OpenAI or replace RAG, but how we can maintain the same performance while cutting down on inference time and cost for your specific use case.</p>
          <p>But let’s face it, the average Joe building RAG applications isn’t confident in their ability to fine-tune an LLM — training data are hard to collect, methodologies are hard to understand, and fine-tuned models are hard to evaluate. And so, fine-tuning has became the best vitamin for LLM practitioners. You’ll often hear excuses such as “Fine-tuning isn’t a priority right now”, “We’ll try with RAG and move to fine-tuning if necessary”, and the classic “Its on the roadmap”. But what if I told you anyone could get started with fine-tuning an LLM in under 2 hours, for free, in under 100 lines of code? Instead of RAG or fine-tuning, why not both?</p>
          <p><a href="https://www.confident-ai.com/blog/the-ultimate-guide-to-fine-tune-llama-2-with-llm-evaluations" target="_blank">Click here for details</a></p>
      </div>
      <hr>
        
      <div id="llama-tutorial" class="section">
        <h2>3. Llama Tutorial</h2>
        <div id="deployment" class="section">
          <h3>3.1 Deployment</h3>
          <p>LLaMA 3 (Large Language Model Meta AI 3) is the third generation of large language model developed by Meta (Facebook). It is an advanced model based on deep learning and artificial intelligence technology, designed to generate high-quality natural language text. LLaMA 3 is commonly used in various natural language processing tasks, including text generation, translation, question answering, text summarization, etc. As an upgraded version after LLaMA 2, it has improved performance, efficiency and processing power, and can better understand and generate complex language structures.</p>
          <p>The LlaMA3 series model group supports 8B and 70B pre-trained versions, and is open source for everyone to use. In the following tutorials, we will deploy and use the dataset to fine-tune the LoRA of llaMA3-8B.</p>
          <p><span class="bold-point">&lt;1&gt;</span><1> Deploy and run on Linux </p>
          <p>With a Linux setup that has a GPU with at least 16GB of VRAM, you should be able to load the 8B Llama model in fp16 natively. If you have an Nvidia GPU, you can confirm your setup using the NVIDIA System Management Interface tool, which will show you what GPU you have, available VRAM, and other useful information, by typing: </p>
        
        </div>
        <div id="fine-tune" class="section">
          <h3>3.2 Fine-tune</h3>
          <p>...</p>
        </div>
      </div>
      <div id="gpt-programming-interface" class="section">
        <h2>4. GPT Programming Interface</h2>
        <p>...</p>
      </div>
      <div id="qa" class="section">
        <h2>5. Q&A</h2>
        <p>...</p>
      </div>
      <div id="contact" class="section">
        <h2>6. Contact</h2>
        <p>...</p>
      </div>
    </main>
  </div>
</div>

<style>
  #sidebar {
    position: fixed;
    top: 70px; /* Adjust this value to move the sidebar down */
    left: 0;
    width: 12%;
    height: 100%;
    z-index: 1;
  }
  main {
    margin-left: -15%;/* Increase margin to move content away from the sidebar */
    width: 130%;
  }
  #sidebar ul {
    padding-left: 0;
  }
  #sidebar .nav-item {
    list-style: none;
    margin-bottom: 10px; /* Add space between items */
  }
  #sidebar .nav-link {
    font-weight: bold; /* Make font bold */
    font-size: 1.1em; /* Increase font size */
    color: #333; /* Dark gray color */
  }
  .sub-item {
    padding-left: 20px;
  }
  .section h2, .section h3, .section h4 {
    text-align: left; /* Align titles to the left */
    margin-left: 0;
    font-weight: bold;
    color: #000; /* Set title color to black */
  }
  .section h2 {
    font-size: 2em; /* Increase size for h2 */
    margin-bottom: 0.75em; /* Increase spacing below h2 */
  }
  .section h3 {
    font-size: 1.75em; /* Increase size for h3 */
    margin-bottom: 0.6em; /* Increase spacing below h3 */
  }
  .section h4 {
    font-size: 1.5em; /* Increase size for h4 */
    margin-bottom: 0.5em; /* Increase spacing below h4 */
  }
  .section {
    padding-left: 0; /* Remove padding to align sections with the left */
    padding-right: 0;
  }
  .section p {
    text-align: left;
  }
</style>
