<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>FITE</title>
    <meta name="description" content="Welcome to the Advanced Wireless Systems Group.  Our group focuses on both experimental and theoretical aspects of research  related to wireless communications and network systems.
">
    
    <!-- Google Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Raleway:300,300i,400,400i,500,500i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i" rel="stylesheet">

    <link rel="stylesheet" href="https://wireless.ie.cuhk.edu.hk/css/newmain.css">

    <!-- Vendor CSS Files -->
    <link href="https://wireless.ie.cuhk.edu.hk/assets/vendor/animate.css/animate.min.css" rel="stylesheet">
    <link href="https://wireless.ie.cuhk.edu.hk/assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://wireless.ie.cuhk.edu.hk/assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
    <link href="https://wireless.ie.cuhk.edu.hk/assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
    <link href="https://wireless.ie.cuhk.edu.hk/assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
    <link href="https://wireless.ie.cuhk.edu.hk/assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

    <!-- Template Main CSS File -->
    <link href="https://wireless.ie.cuhk.edu.hk/assets/css/style.css" rel="stylesheet">

    <link rel="canonical" href="https://wireless.ie.cuhk.edu.hk/fite/">
    <link rel="icon" type ="image/x-icon" href="https://wireless.ie.cuhk.edu.hk/images/favicon.ico">
  
  
  
</head>
  

  <body>

    <!-- ======= Header ======= -->
<header id="header" class="d-flex align-items-center">
    <div class="container d-flex justify-content-between align-items-center">

      <div class="logo">
        <!-- <h1><a href="index.html">AWSG</a></h1> -->
        <!-- Uncomment below if you prefer to use an image logo -->
        <a href="index.html"><img src="https://wireless.ie.cuhk.edu.hk/images/logo-text.png" alt="" class="img-fluid"></a>
      </div>

      <nav id="navbar" class="navbar">
        <ul>
            <li><a href="https://wireless.ie.cuhk.edu.hk/">Home</a></li>
            <li><a href="https://wireless.ie.cuhk.edu.hk/team.html">Team</a></li>		
            <li><a href="https://wireless.ie.cuhk.edu.hk/publications.html">Publications</a></li>
            <li class="dropdown"><a href="https://wireless.ie.cuhk.edu.hk/research.html"><span>Research</span> <i class="bi bi-chevron-down"></i></a>
                <ul>
                  <li><a href="https://wireless.ie.cuhk.edu.hk/research.html#ai">AI-driven wireless systems</a></li>
                  <li><a href="https://wireless.ie.cuhk.edu.hk/research.html#iiot">Industrial Wireless Networks for IIoT</a></li>
                  <li><a href="https://wireless.ie.cuhk.edu.hk/research.html#blockchain">Networking for Blockchain</a></li>
                </ul>
            </li>
            <li><a href="https://wireless.ie.cuhk.edu.hk/allnews.html">News</a></li>
            <li><a href="https://wireless.ie.cuhk.edu.hk/fite/" target="_blank">FITE</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
</header><!-- End Header -->


    <!-- ======= Breadcrumbs ======= -->
<section id="breadcrumbs" class="breadcrumbs" style="background: url(https://wireless.ie.cuhk.edu.hk/images/slider7001400/circuit_trans_cut.jpg); background-color: rgba(120, 120, 120, 0.5);">
    <div class="container">

      <ol>
        <li><a href="index.html">Home</a></li>
        <li>FITE</li>
      </ol>
      <h2>FITE</h2>

    </div>
</section><!-- End Breadcrumbs -->

    <main id="main" style="margin-top: 15px;">
        <div class="container-fluid">
            <div class="row">
              <div class="col-lg-2">
              </div>
              <div class="col-lg-8">
                  <div class="container-fluid">
  <div class="row">
    <nav id="sidebar" class="col-md-3 col-lg-2 d-md-block bg-light sidebar">
      <div class="position-sticky">
        <ul class="nav flex-column">
          <li class="nav-item">
            <a class="nav-link active" href="#about-fite-program">1. About FITE Program</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#technical-sharing">2. Technical Sharing</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-1">2.1 Share 1</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-2">2.2 Share 2</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#share-3">2.3 Share 3</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#llama-tutorial">3. Llama Tutorial</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#deployment">3.1 Deployment</a>
          </li>
          <li class="nav-item sub-item">
            <a class="nav-link" href="#fine-tune">3.2 Fine-tune</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#gpt-programming-interface">4. GPT Programming Interface</a></li>
          <li class="nav-item">
            <a class="nav-link" href="#qa">5. Q&A</a></li>
          <li class="nav-item">
            <a class="nav-link" href="#contact">6. Contact</a></li>
        </ul>
      </div>
    </nav>

    <main class="col-md-9 ms-sm-auto col-lg-10 px-md-4">
      <div id="about-fite-program" class="section">
        <h2>1. About FITE Program</h2>
        <p>...</p>
      </div>
      <hr>

      <div id="technical-sharing" class="section">
        <h2>2. Technical Sharing</h2>
        <div id="share-1" class="section">
          <h3>2.1 Share 1</h3>
          <h4>Fine-tune Llama 3.1 Ultra-Efficiently with Unsloth</h4>
          <p><img src="/images/respic/share1.jpg" class="img-responsive" width="60%" /></p>
          <p>The recent release of Llama 3.1 offers models with an incredible level of performance, closing the gap between closed-source and open-weight models. Instead of using frozen, general-purpose LLMs like GPT-4o and Claude 3.5, you can fine-tune Llama 3.1 for your specific use cases to achieve better performance and customizability at a lower cost.</p>
          <p><a href="https://huggingface.co/blog/mlabonne/sft-llama3" target="_blank">Click here for details</a></p>
          <div style="margin-top: 20px;">
            <iframe width="560" height="315" src="https://www.youtube.com/embed/rpAtVIZB72U" frameborder="0" allowfullscreen></iframe>
          </div>
        </div>
        <hr>
        
        <div id="share-2" class="section">
          <h3>2.2 Share 2</h3>
          <h4>Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora</h4>
          <p><img src="/images/respic/share2.jpg" class="img-responsive" width="60%" /></p>
          <p>Open LLMs like Meta Llama 3, Mistral AI Mistral & Mixtral models or AI21 Jamba are now OpenAI competitors. However, most of the time you need to fine-tune the model on your data to unlock the full potential of the model. Fine-tuning smaller LLMs, like Mistral became very accessible on a single GPU by using Q-Lora. But efficiently fine-tuning bigger models like Llama 3 70b or Mixtral stayed a challenge until now.</p>
          <p>This blog post walks you thorugh how to fine-tune a Llama 3 using PyTorch FSDP and Q-Lora with the help of Hugging Face TRL, Transformers, peft & datasets. In addition to FSDP we will use Flash Attention v2 through the Pytorch SDPA implementation.</p>
          <p><a href="https://www.philschmid.de/fsdp-qlora-llama3" target="_blank">Click here for details</a></p>
        </div>
        <hr>
        
        <div id="share-3" class="section">
          <h3>2.3 Share 3</h3>
          <h4>The Ultimate Guide to Fine-Tune LLaMA 3, With LLM Evaluations</h4>
          <p><img src="/images/respic/share3.jpg" class="img-responsive" width="50%" /></p>
          <p>Fine-tuning a Large Language Model (LLM) comes with tons of benefits when compared to relying on proprietary foundational models such as OpenAI’s GPT models. Think about it, you get 10x cheaper inference cost, 10x faster tokens per second, and not have to worry about any shady stuff OpenAI’s doing behind their APIs. The way everyone should be thinking about fine-tuning, is not how we can outperform OpenAI or replace RAG, but how we can maintain the same performance while cutting down on inference time and cost for your specific use case.</p>
          <p>But let’s face it, the average Joe building RAG applications isn’t confident in their ability to fine-tune an LLM — training data are hard to collect, methodologies are hard to understand, and fine-tuned models are hard to evaluate. And so, fine-tuning has became the best vitamin for LLM practitioners. You’ll often hear excuses such as “Fine-tuning isn’t a priority right now”, “We’ll try with RAG and move to fine-tuning if necessary”, and the classic “Its on the roadmap”. But what if I told you anyone could get started with fine-tuning an LLM in under 2 hours, for free, in under 100 lines of code? Instead of RAG or fine-tuning, why not both?</p>
          <p><a href="https://www.confident-ai.com/blog/the-ultimate-guide-to-fine-tune-llama-2-with-llm-evaluations" target="_blank">Click here for details</a></p>
      </div>
      <hr>
        
      <div id="llama-tutorial" class="section">
        <h2>3. Llama Tutorial</h2>
        <div id="deployment" class="section">
          <h3>3.1 Deployment</h3>
          <p>LLaMA 3 (Large Language Model Meta AI 3) is the third generation of large language model developed by Meta (Facebook). It is an advanced model based on deep learning and artificial intelligence technology, designed to generate high-quality natural language text. LLaMA 3 is commonly used in various natural language processing tasks, including text generation, translation, question answering, text summarization, etc. As an upgraded version after LLaMA 2, it has improved performance, efficiency and processing power, and can better understand and generate complex language structures.</p>
          <p>The LlaMA3 series model group supports 8B and 70B pre-trained versions, and is open source for everyone to use. In the following tutorials, we will deploy and use the dataset to fine-tune the LoRA of llaMA3-8B.</p>
          <p><strong>&lt;1&gt; Deploy and run on Linux</strong></p>
          <p>With a Linux setup that has a GPU with at least 16GB of VRAM, you should be able to load the 8B Llama model in fp16 natively. If you have an Nvidia GPU, you can confirm your setup using the NVIDIA System Management Interface tool, which will show you what GPU you have, available VRAM, and other useful information,  </p>
          <p>by typing: <code>nvidia-smi</code></p>
          <p>to view nvidia's VRAM configuration.</p>
          <p><img src="/images/respic/tut1.png" class="img-responsive" width="50%" /></p>
          <p><strong>&lt;2&gt; Get llama3 weight</strong></p>
          <p>Visit the <a href="https://llama.meta.com/llama-downloads/" target="_blank">llama website</a>, fill in your details in the form, and select the model you want to download.</p>
          <p>Read and agree to the license agreement, then click Accept and Continue. You will see a unique URL on the website. You will also receive the URL in an email, which will be valid for 24 hours and allow you to download each model up to 5 times. You can request a new URL at any time.</p>
          <p>Now, we are ready to get the weights and run the model locally on our machine. It is recommended to use a Python virtual environment to run this demo. In this demo, we use Miniconda, but you can use any virtual environment of your choice. In addition, please refer to the configuration instructions of <a href="https://docs.anaconda.com/miniconda/miniconda-install/" target="_blank">miniconda</a>:</p>
          <p><img src="/images/respic/tut2.png" class="img-responsive" width="50%" /></p>
          <p>Open your terminal and create a new folder called llama3-demo in your workspace. Navigate to the new folder and clone the Llama repository:</p>
          <p><code>mkdir llama3-demo</code></p>
          <p><code>cd llama3-demo</code></p>
          <p><code>git clone https://github.com/meta-llama/llama3.git</code></p>
          <p><img src="/images/respic/tut3.png" class="img-responsive" width="50%" /></p>
          <p>For this demo, we need to install two prerequisites: wget and md5sum. To confirm if your distribution has these, use:</p>
          <p><code>wget --version</code></p>
          <p><code>md5sum –version</code></p>
          <p>It should return the installed version. If your distribution does not have these, you can install them with the following commands:</p>
          <p><code>apt-get install wget</code></p>
          <p><code>apt-get install md5sum</code></p>
          <p>To make sure we have all package dependencies installed, in the newly cloned repo folder enter:</p>
          <p><code>pip install -e </code></p>
          <p><img src="/images/respic/tut4.png" class="img-responsive" width="50%" /></p>
          <p>Now, we are ready to download the model weights for our local setup. Our team created a helper script to make it easy to download the model weights. In your terminal, enter:</p>
          <p><code>./download.sh</code></p>
          <p><img src="/images/respic/tut5.png" class="img-responsive" width="50%" /></p>
          <p>The script will ask you for the URL from your email. Paste the URL you received from Meta. It will then ask you to enter a list of models to download. In our example, we will download the 8B pretrained model and the fine-tuned 8B chat model. Therefore, we will enter "8B,8B-instruct".</p>
          <p><strong>&lt;3&gt; Running the Model</strong></p>
          <p>We are ready to run an example inference script to test that our model is setup correctly and working properly. We can use an example Python script called <a href="https://github.com/meta-llama/llama3/blob/main/example_text_completion.py" target="_blank">example_text_completion.py</a> to test the model. </p>
          <p>To run the script, return to our terminal and type in the llama3repo:</p>
          <p><code>torchrun --nproc_per_node 1 example_text_completion.py </code></p>
          <p><code>-ckpt_dir Meta-Llama-3-8B/ </code></p>
          <p><code>-tokenizer_path Meta-Llama-3-8B/tokenizer.model </code></p>
          <p><code>-max_seq_len 128 </code></p>
          <p><code>-max_batch_size 4</code></p>
          <p>Replace Meta-Llama-3-8B/ with the path to your checkpoint directory and tokenizer.model with the path to your tokenizer model. If you run it from this main directory, the paths may not need to be changed.</p>
          <p>Set –nproc_per_node to the MP value for the model you are using. For the 8B model, this value is set to 1.</p>
          <p>Adjust the max_seq_len and max_batch_size parameters as needed. We set them to 128 and 4 respectively. </p>
          <p><img src="/images/respic/tut6.png" class="img-responsive" width="50%" /></p>
          <p><strong>&lt;4&gt; Trying out and improving 8b-instruct</strong></p>
          <p>To try out the fine-tuned chat model (8B-instruct), we have a similar example called <a href="https://github.com/meta-llama/llama3/blob/main/example_chat_completion.py" target="_blank">example_chat_completion.py</a>.</p>
          <p><code>torchrun --nproc_per_node 1 example_chat_completion.py </code></p>
          <p><code>-ckpt_dir Meta-Llama-3-8B-Instruct/ </code></p>
          <p><code>-tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model </code></p>
          <p><code>-max_seq_len 512 </code></p>
          <p><code>-max_batch_size 6</code></p>
          p><img src="/images/respic/tut7.png" class="img-responsive" width="50%" /></p>
          p><img src="/images/respic/tut8.png" class="img-responsive" width="50%" /></p>
          <p>Note that in this case, we use the Meta-Llama-3-8B-Instruct/model and provide the correct tokenizer instruction under the model folder.</p>
        </div>
        <div id="fine-tune" class="section">
          <h3>3.2 Fine-tune</h3>
          <p>...</p>
        </div>
      </div>
      <div id="gpt-programming-interface" class="section">
        <h2>4. GPT Programming Interface</h2>
        <p>...</p>
      </div>
      <div id="qa" class="section">
        <h2>5. Q&A</h2>
        <p>...</p>
      </div>
      <div id="contact" class="section">
        <h2>6. Contact</h2>
        <p>...</p>
      </div>
    </main>
  </div>
</div>

<style>
  #sidebar {
    position: fixed;
    top: 70px; /* Adjust this value to move the sidebar down */
    left: 0;
    width: 12%;
    height: 100%;
    z-index: 1;
  }
  main {
    margin-left: -15%;/* Increase margin to move content away from the sidebar */
    width: 130%;
  }
  #sidebar ul {
    padding-left: 0;
  }
  #sidebar .nav-item {
    list-style: none;
    margin-bottom: 10px; /* Add space between items */
  }
  #sidebar .nav-link {
    font-weight: bold; /* Make font bold */
    font-size: 1.1em; /* Increase font size */
    color: #333; /* Dark gray color */
  }
  .sub-item {
    padding-left: 20px;
  }
  .section h2, .section h3, .section h4 {
    text-align: left; /* Align titles to the left */
    margin-left: 0;
    font-weight: bold;
    color: #000; /* Set title color to black */
  }
  .section h2 {
    font-size: 2em; /* Increase size for h2 */
    margin-bottom: 0.75em; /* Increase spacing below h2 */
  }
  .section h3 {
    font-size: 1.75em; /* Increase size for h3 */
    margin-bottom: 0.6em; /* Increase spacing below h3 */
  }
  .section h4 {
    font-size: 1.5em; /* Increase size for h4 */
    margin-bottom: 0.5em; /* Increase spacing below h4 */
  }
  .section {
    padding-left: 0; /* Remove padding to align sections with the left */
    padding-right: 0;
  }
  .section p {
    text-align: left;
  }
</style>

              </div>
              <div class="col-lg-2">
              </div>
            </div>
        </div>
    </main>
    

    <footer id="footer">
  <div class="footer-top">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-lg-3 col-md-6 footer-info">
		</div>
		<div class="col-lg-3 col-md-6 footer-info">
			  We are part of the <a href="https://www.ie.cuhk.edu.hk/main/index.shtml">IE Department</a> <br/>
				at <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>.
		</div>
		<div class="col-lg-3 col-md-6 footer-info">
		  Location:<br />
		  Rm729, SHB<br />
		  The Chinese University of Hong Kong<br />
		  Shatin, H.K.<br />		  
            (<a href="https://goo.gl/maps/6gDa8HKdC8TTGoQCA">Maps</a>)
		</div>
		<div class="col-lg-3 col-md-6 footer-info">
		</div>
	  </div>
	</div>
  </div>
  <div class="container">
	<div class="credits">
		Designed by Jiaxin and generated by Jekyll.
	</div>
  </div>
</footer>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<!-- <script src="https://wireless.ie.cuhk.edu.hk/js/bootstrap.min.js"></script> -->

    <!-- Vendor JS Files -->
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/glightbox/js/glightbox.min.js"></script>
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/php-email-form/validate.js"></script>
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/purecounter/purecounter.js"></script>
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/swiper/swiper-bundle.min.js"></script>
<script src="https://wireless.ie.cuhk.edu.hk/assets/vendor/waypoints/noframework.waypoints.js"></script>

<!-- Template Main JS File -->
<script src="https://wireless.ie.cuhk.edu.hk/assets/js/main.js"></script>

  </body>

</html>
